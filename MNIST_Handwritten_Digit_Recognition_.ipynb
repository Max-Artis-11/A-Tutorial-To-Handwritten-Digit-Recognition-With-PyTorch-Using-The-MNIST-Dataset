{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch: A Tutorial To Handwritten Digit Recognition With PyTorch Using The MNIST Dataset"
      ],
      "metadata": {
        "id": "hx5W7U0lP7ov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PQGuVS2P0Flp"
      },
      "outputs": [],
      "source": [
        "# Gives Us Access To The MNIST Dataset\n",
        "from torchvision import datasets\n",
        "\n",
        "# Transforms is a module within torchvision that contains functions to apply image transformations. These transformations are useful for data augmentation or preprocessing.\n",
        "# In this case we use ToTensor which is a specific transformation that converts a PIL (Python Imaging Library) image or a NumPy ndarray into a PyTorch tensor.\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(root = 'data', train = True, transform = ToTensor(), download = True)\n",
        "\n",
        "test_data = datasets.MNIST(root = 'data', train = False, transform = ToTensor(), download = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Gw-8bL1UE1",
        "outputId": "e612550d-1487-4cdf-e235-df8bf642d3ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 340kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.15MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Details of the Dataset\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEjhKiLO1laW",
        "outputId": "7ddf0ede-aa4b-451e-a101-0cbd63a04270"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Details of the Dataset\n",
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDFfQ8TS1pC2",
        "outputId": "b509ebc1-74f4-4e35-9b51-cdd394ac49c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Shape. Format: Amount of images, width, height\n",
        "train_data.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0vIgGD51qtS",
        "outputId": "b34bbb48-ca0b-4d07-e276-45391394f006"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Shape. Format: Amount of images, width, height\n",
        "test_data.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXl122iF11sK",
        "outputId": "58d5d229-8233-46ef-8428-844712894284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many Images\n",
        "train_data.targets.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aeluSPk15jq",
        "outputId": "c2ec0d2d-b77d-4189-b20d-c43fb3bb1bb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Digits from 0 - 9\n",
        "train_data.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH6r5ENd1-mL",
        "outputId": "a51e4367-d2f3-4d62-839f-97d9f8c9bc2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4,  ..., 5, 6, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the DataLoader class from PyTorch's torch.utils.data module\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a dictionary to hold DataLoader objects for both train and test data\n",
        "loaders = {\n",
        "\n",
        "    # Define a DataLoader for the training data\n",
        "    'train': DataLoader(train_data,  # The dataset to load (train_data is the training dataset)\n",
        "                        batch_size=100,  # The size of each batch (100 samples per batch)\n",
        "                        shuffle=True,  # Shuffle the dataset so the order is random each epoch\n",
        "                        num_workers=1),  # Number of CPU cores to use for loading the data. Use 1 here.\n",
        "\n",
        "    # Define a DataLoader for the test data (similar to train but for testing purposes)\n",
        "    'test': DataLoader(train_data,  # The dataset to load (test_data should be the testing dataset)\n",
        "                       batch_size=100,  # Same batch size as for training (100 samples per batch)\n",
        "                       shuffle=True,  # Shuffle the test data to prevent order bias\n",
        "                       num_workers=1)  # Same as for training: use 1 CPU core to load the data\n",
        "}\n"
      ],
      "metadata": {
        "id": "dUwp8sE12BfY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print The DataLoaders\n",
        "loaders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKFoBWcd3khw",
        "outputId": "cb424b93-9697-471d-dddf-6247f120d3b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': <torch.utils.data.dataloader.DataLoader at 0x7ba63d4a7550>,\n",
              " 'test': <torch.utils.data.dataloader.DataLoader at 0x7ba63d4a6190>}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn            # Import the neural network module from PyTorch\n",
        "import torch.nn.functional as F  # Import common functions like ReLU, max pooling, etc.\n",
        "import torch.optim as optim      # Import optimization algorithms (e.g., SGD, Adam)\n",
        "\n",
        "# Define the CNN class, which inherits from nn.Module (base class for all neural networks in PyTorch)\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    # Initialize the layers of the network\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Define the first convolutional layer\n",
        "        # This will take 1 input channel (grayscale images) and produce 10 output channels (feature maps)\n",
        "        # The kernel size (filter size) is 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "\n",
        "        # Define the second convolutional layer\n",
        "        # This will take 10 input channels (output of the previous layer) and produce 20 output channels\n",
        "        # Kernel size is also 5x5\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        # This randomly sets some of the outputs to zero during training (with a default dropout rate of 0.5)\n",
        "        self.conv2_drop = nn.Dropout()\n",
        "\n",
        "        # Define the first fully connected (linear) layer\n",
        "        # This will take 320 input features and produce 50 output features\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "\n",
        "        # Define the second fully connected (linear) layer\n",
        "        # This will take 50 input features and produce 10 output features (the number of classes for classification)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    # Define the forward pass of the network\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Apply the first convolutional layer followed by ReLU activation and 2x2 max pooling\n",
        "        # Conv1: Detect basic features like edges, and max pooling reduces the spatial size (downsampling)\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "\n",
        "        # Apply the second convolutional layer followed by dropout, ReLU activation, and 2x2 max pooling\n",
        "        # Conv2: Detects more complex features, dropout helps prevent overfitting, and max pooling downsamples\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "\n",
        "        # Flatten the 2D feature maps from the convolutional layers into a 1D vector\n",
        "        # -1 means the batch size is inferred automatically, and 320 is the total number of features\n",
        "        # This is necessary to connect the convolutional layers to the fully connected layers\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        # Pass the flattened features through the first fully connected layer followed by ReLU activation\n",
        "        # The output of this layer is 50 features\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Apply dropout during training to prevent overfitting\n",
        "        # During evaluation (testing), dropout is not applied\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        # Pass the output through the second fully connected layer to get the final class scores (logits)\n",
        "        # The output of this layer is 10 values (one for each class)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Apply softmax to the final output to get class probabilities\n",
        "        # Softmax converts raw scores into probabilities that sum to 1\n",
        "        return F.softmax(x, dim=1)  # dim=1 ensures softmax is applied across each sample's features (not across batches)"
      ],
      "metadata": {
        "id": "Wqfr90cT3urD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch library for tensor operations and deep learning functionalities\n",
        "import torch\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# - If CUDA is available (i.e., a compatible GPU), the model will run on the GPU (faster computation).\n",
        "# - If CUDA is not available, the code will use the CPU (which is slower for deep learning tasks).\n",
        "\n",
        "# Initialize the CNN model and move it to the selected device (GPU or CPU)\n",
        "model = CNN().to(device)\n",
        "# - `CNN()` initializes the CNN model (which is defined elsewhere in the code).\n",
        "# - `.to(device)` moves the model to either GPU or CPU depending on availability.\n",
        "\n",
        "# Set up the Adam optimizer with the model's parameters and a learning rate of 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# - `model.parameters()` tells the optimizer which parameters (weights and biases) to optimize.\n",
        "# - Adam is an adaptive optimizer, and it's a popular choice for training deep learning models.\n",
        "# - `lr=0.001` is the learning rate, controlling the step size of each update during training.\n",
        "\n",
        "# Define the loss function as CrossEntropyLoss, suitable for multi-class classification\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# - CrossEntropyLoss is commonly used for classification tasks where each input sample belongs to one of several classes.\n",
        "# - This function calculates the difference between the predicted output and the true target values.\n",
        "\n",
        "# Function to train the model for one epoch (one full pass through the training data)\n",
        "def train(epoch):\n",
        "    model.train()  # Set the model to training mode (important for layers like dropout/batchnorm)\n",
        "\n",
        "    # Loop over the training data in batches\n",
        "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "        # Move the data and target labels to the same device as the model (either GPU or CPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Clear previous gradients from the optimizer (they accumulate by default)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform a forward pass: get model predictions for the current batch of data\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate the loss (how far off the predictions are from the actual labels)\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Perform a backward pass: compute gradients of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters using the optimizer (based on the gradients)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Every 20 batches, print the progress (helpful for monitoring training)\n",
        "        if batch_idx % 20 == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders['train'].dataset)} \"\n",
        "                  f\"({100. * batch_idx / len(loaders['train']):.0f}%)]\\t{loss.item():.6f}\")\n",
        "            # - This prints the current epoch, how many samples have been processed, and the current loss for every 20 batches.\n",
        "\n",
        "# Function to test (evaluate) the model's performance on the test dataset\n",
        "def test():\n",
        "    model.eval()  # Set the model to evaluation mode (important for layers like dropout/batchnorm)\n",
        "\n",
        "    # Initialize variables to keep track of test loss and accuracy\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Disable gradient computation (since we're not training, this saves memory and speeds up inference)\n",
        "    with torch.no_grad():\n",
        "        # Loop over the test data in batches\n",
        "        for data, target in loaders['test']:\n",
        "            # Move the test data and target labels to the same device as the model\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Get model predictions for the current batch\n",
        "            output = model(data)\n",
        "\n",
        "            # Add the loss of the current batch to the total test loss\n",
        "            test_loss += loss_fn(output, target).item()\n",
        "\n",
        "            # Get the predicted class (the class with the highest predicted score)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            # Count how many predictions were correct for the current batch\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Calculate average test loss (mean over all test data)\n",
        "    test_loss /= len(loaders['test'].dataset)\n",
        "\n",
        "    # Print the test results: average loss and accuracy\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy {correct}/{len(loaders['test'].dataset)} \"\n",
        "          f\"({100. * correct / len(loaders['test'].dataset):.0f}%)\\n\")\n",
        "    # - This shows the average loss over all test samples and the overall classification accuracy."
      ],
      "metadata": {
        "id": "WPX6cN0dA0xz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the training and testing process for 10 epochs\n",
        "for epoch in range(1, 11):\n",
        "    # Call the 'train' function to train the model for the current epoch\n",
        "    train(epoch)\n",
        "\n",
        "    # After training the model for the current epoch, evaluate the model's performance on the test set\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgFnVNdtFBia",
        "outputId": "c163b571-466e-4bb2-d2b1-d2d58217d77d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\t2.301117\n",
            "Train Epoch: 1 [2000/60000 (3%)]\t2.261086\n",
            "Train Epoch: 1 [4000/60000 (7%)]\t2.155995\n",
            "Train Epoch: 1 [6000/60000 (10%)]\t1.986563\n",
            "Train Epoch: 1 [8000/60000 (13%)]\t1.890035\n",
            "Train Epoch: 1 [10000/60000 (17%)]\t1.766249\n",
            "Train Epoch: 1 [12000/60000 (20%)]\t1.747553\n",
            "Train Epoch: 1 [14000/60000 (23%)]\t1.764811\n",
            "Train Epoch: 1 [16000/60000 (27%)]\t1.708770\n",
            "Train Epoch: 1 [18000/60000 (30%)]\t1.792212\n",
            "Train Epoch: 1 [20000/60000 (33%)]\t1.666129\n",
            "Train Epoch: 1 [22000/60000 (37%)]\t1.634328\n",
            "Train Epoch: 1 [24000/60000 (40%)]\t1.691413\n",
            "Train Epoch: 1 [26000/60000 (43%)]\t1.651315\n",
            "Train Epoch: 1 [28000/60000 (47%)]\t1.657513\n",
            "Train Epoch: 1 [30000/60000 (50%)]\t1.620328\n",
            "Train Epoch: 1 [32000/60000 (53%)]\t1.608250\n",
            "Train Epoch: 1 [34000/60000 (57%)]\t1.640387\n",
            "Train Epoch: 1 [36000/60000 (60%)]\t1.611785\n",
            "Train Epoch: 1 [38000/60000 (63%)]\t1.649751\n",
            "Train Epoch: 1 [40000/60000 (67%)]\t1.613431\n",
            "Train Epoch: 1 [42000/60000 (70%)]\t1.638209\n",
            "Train Epoch: 1 [44000/60000 (73%)]\t1.553444\n",
            "Train Epoch: 1 [46000/60000 (77%)]\t1.564958\n",
            "Train Epoch: 1 [48000/60000 (80%)]\t1.638289\n",
            "Train Epoch: 1 [50000/60000 (83%)]\t1.587421\n",
            "Train Epoch: 1 [52000/60000 (87%)]\t1.597070\n",
            "Train Epoch: 1 [54000/60000 (90%)]\t1.637828\n",
            "Train Epoch: 1 [56000/60000 (93%)]\t1.654674\n",
            "Train Epoch: 1 [58000/60000 (97%)]\t1.546511\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy 56361/60000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\t1.583639\n",
            "Train Epoch: 2 [2000/60000 (3%)]\t1.557618\n",
            "Train Epoch: 2 [4000/60000 (7%)]\t1.589332\n",
            "Train Epoch: 2 [6000/60000 (10%)]\t1.553445\n",
            "Train Epoch: 2 [8000/60000 (13%)]\t1.574772\n",
            "Train Epoch: 2 [10000/60000 (17%)]\t1.505321\n",
            "Train Epoch: 2 [12000/60000 (20%)]\t1.634545\n",
            "Train Epoch: 2 [14000/60000 (23%)]\t1.579059\n",
            "Train Epoch: 2 [16000/60000 (27%)]\t1.563730\n",
            "Train Epoch: 2 [18000/60000 (30%)]\t1.545429\n",
            "Train Epoch: 2 [20000/60000 (33%)]\t1.580686\n",
            "Train Epoch: 2 [22000/60000 (37%)]\t1.601115\n",
            "Train Epoch: 2 [24000/60000 (40%)]\t1.582394\n",
            "Train Epoch: 2 [26000/60000 (43%)]\t1.528720\n",
            "Train Epoch: 2 [28000/60000 (47%)]\t1.570483\n",
            "Train Epoch: 2 [30000/60000 (50%)]\t1.587923\n",
            "Train Epoch: 2 [32000/60000 (53%)]\t1.571795\n",
            "Train Epoch: 2 [34000/60000 (57%)]\t1.586367\n",
            "Train Epoch: 2 [36000/60000 (60%)]\t1.528550\n",
            "Train Epoch: 2 [38000/60000 (63%)]\t1.545333\n",
            "Train Epoch: 2 [40000/60000 (67%)]\t1.537636\n",
            "Train Epoch: 2 [42000/60000 (70%)]\t1.548587\n",
            "Train Epoch: 2 [44000/60000 (73%)]\t1.568820\n",
            "Train Epoch: 2 [46000/60000 (77%)]\t1.550594\n",
            "Train Epoch: 2 [48000/60000 (80%)]\t1.547768\n",
            "Train Epoch: 2 [50000/60000 (83%)]\t1.547605\n",
            "Train Epoch: 2 [52000/60000 (87%)]\t1.534037\n",
            "Train Epoch: 2 [54000/60000 (90%)]\t1.548542\n",
            "Train Epoch: 2 [56000/60000 (93%)]\t1.582497\n",
            "Train Epoch: 2 [58000/60000 (97%)]\t1.547949\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy 57423/60000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\t1.552985\n",
            "Train Epoch: 3 [2000/60000 (3%)]\t1.520649\n",
            "Train Epoch: 3 [4000/60000 (7%)]\t1.551885\n",
            "Train Epoch: 3 [6000/60000 (10%)]\t1.506101\n",
            "Train Epoch: 3 [8000/60000 (13%)]\t1.526435\n",
            "Train Epoch: 3 [10000/60000 (17%)]\t1.544796\n",
            "Train Epoch: 3 [12000/60000 (20%)]\t1.530941\n",
            "Train Epoch: 3 [14000/60000 (23%)]\t1.576092\n",
            "Train Epoch: 3 [16000/60000 (27%)]\t1.525777\n",
            "Train Epoch: 3 [18000/60000 (30%)]\t1.530167\n",
            "Train Epoch: 3 [20000/60000 (33%)]\t1.525811\n",
            "Train Epoch: 3 [22000/60000 (37%)]\t1.506245\n",
            "Train Epoch: 3 [24000/60000 (40%)]\t1.544090\n",
            "Train Epoch: 3 [26000/60000 (43%)]\t1.544342\n",
            "Train Epoch: 3 [28000/60000 (47%)]\t1.553301\n",
            "Train Epoch: 3 [30000/60000 (50%)]\t1.533216\n",
            "Train Epoch: 3 [32000/60000 (53%)]\t1.609927\n",
            "Train Epoch: 3 [34000/60000 (57%)]\t1.518648\n",
            "Train Epoch: 3 [36000/60000 (60%)]\t1.545590\n",
            "Train Epoch: 3 [38000/60000 (63%)]\t1.537044\n",
            "Train Epoch: 3 [40000/60000 (67%)]\t1.539211\n",
            "Train Epoch: 3 [42000/60000 (70%)]\t1.508253\n",
            "Train Epoch: 3 [44000/60000 (73%)]\t1.519886\n",
            "Train Epoch: 3 [46000/60000 (77%)]\t1.505304\n",
            "Train Epoch: 3 [48000/60000 (80%)]\t1.570948\n",
            "Train Epoch: 3 [50000/60000 (83%)]\t1.552514\n",
            "Train Epoch: 3 [52000/60000 (87%)]\t1.549738\n",
            "Train Epoch: 3 [54000/60000 (90%)]\t1.559166\n",
            "Train Epoch: 3 [56000/60000 (93%)]\t1.525835\n",
            "Train Epoch: 3 [58000/60000 (97%)]\t1.574917\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy 58038/60000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\t1.564669\n",
            "Train Epoch: 4 [2000/60000 (3%)]\t1.516856\n",
            "Train Epoch: 4 [4000/60000 (7%)]\t1.546470\n",
            "Train Epoch: 4 [6000/60000 (10%)]\t1.485103\n",
            "Train Epoch: 4 [8000/60000 (13%)]\t1.487868\n",
            "Train Epoch: 4 [10000/60000 (17%)]\t1.517557\n",
            "Train Epoch: 4 [12000/60000 (20%)]\t1.526893\n",
            "Train Epoch: 4 [14000/60000 (23%)]\t1.524497\n",
            "Train Epoch: 4 [16000/60000 (27%)]\t1.526541\n",
            "Train Epoch: 4 [18000/60000 (30%)]\t1.519880\n",
            "Train Epoch: 4 [20000/60000 (33%)]\t1.524829\n",
            "Train Epoch: 4 [22000/60000 (37%)]\t1.495767\n",
            "Train Epoch: 4 [24000/60000 (40%)]\t1.516649\n",
            "Train Epoch: 4 [26000/60000 (43%)]\t1.527836\n",
            "Train Epoch: 4 [28000/60000 (47%)]\t1.548590\n",
            "Train Epoch: 4 [30000/60000 (50%)]\t1.517138\n",
            "Train Epoch: 4 [32000/60000 (53%)]\t1.505982\n",
            "Train Epoch: 4 [34000/60000 (57%)]\t1.567314\n",
            "Train Epoch: 4 [36000/60000 (60%)]\t1.539017\n",
            "Train Epoch: 4 [38000/60000 (63%)]\t1.579367\n",
            "Train Epoch: 4 [40000/60000 (67%)]\t1.523286\n",
            "Train Epoch: 4 [42000/60000 (70%)]\t1.489609\n",
            "Train Epoch: 4 [44000/60000 (73%)]\t1.498527\n",
            "Train Epoch: 4 [46000/60000 (77%)]\t1.526128\n",
            "Train Epoch: 4 [48000/60000 (80%)]\t1.487472\n",
            "Train Epoch: 4 [50000/60000 (83%)]\t1.497660\n",
            "Train Epoch: 4 [52000/60000 (87%)]\t1.515026\n",
            "Train Epoch: 4 [54000/60000 (90%)]\t1.558335\n",
            "Train Epoch: 4 [56000/60000 (93%)]\t1.531270\n",
            "Train Epoch: 4 [58000/60000 (97%)]\t1.492673\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy 58143/60000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\t1.516000\n",
            "Train Epoch: 5 [2000/60000 (3%)]\t1.524995\n",
            "Train Epoch: 5 [4000/60000 (7%)]\t1.504780\n",
            "Train Epoch: 5 [6000/60000 (10%)]\t1.490787\n",
            "Train Epoch: 5 [8000/60000 (13%)]\t1.545367\n",
            "Train Epoch: 5 [10000/60000 (17%)]\t1.486483\n",
            "Train Epoch: 5 [12000/60000 (20%)]\t1.503826\n",
            "Train Epoch: 5 [14000/60000 (23%)]\t1.490419\n",
            "Train Epoch: 5 [16000/60000 (27%)]\t1.514090\n",
            "Train Epoch: 5 [18000/60000 (30%)]\t1.521664\n",
            "Train Epoch: 5 [20000/60000 (33%)]\t1.509965\n",
            "Train Epoch: 5 [22000/60000 (37%)]\t1.494937\n",
            "Train Epoch: 5 [24000/60000 (40%)]\t1.539349\n",
            "Train Epoch: 5 [26000/60000 (43%)]\t1.501068\n",
            "Train Epoch: 5 [28000/60000 (47%)]\t1.537508\n",
            "Train Epoch: 5 [30000/60000 (50%)]\t1.497508\n",
            "Train Epoch: 5 [32000/60000 (53%)]\t1.529017\n",
            "Train Epoch: 5 [34000/60000 (57%)]\t1.527070\n",
            "Train Epoch: 5 [36000/60000 (60%)]\t1.506344\n",
            "Train Epoch: 5 [38000/60000 (63%)]\t1.519411\n",
            "Train Epoch: 5 [40000/60000 (67%)]\t1.538934\n",
            "Train Epoch: 5 [42000/60000 (70%)]\t1.558041\n",
            "Train Epoch: 5 [44000/60000 (73%)]\t1.529334\n",
            "Train Epoch: 5 [46000/60000 (77%)]\t1.498991\n",
            "Train Epoch: 5 [48000/60000 (80%)]\t1.493858\n",
            "Train Epoch: 5 [50000/60000 (83%)]\t1.506175\n",
            "Train Epoch: 5 [52000/60000 (87%)]\t1.512719\n",
            "Train Epoch: 5 [54000/60000 (90%)]\t1.494298\n",
            "Train Epoch: 5 [56000/60000 (93%)]\t1.507666\n",
            "Train Epoch: 5 [58000/60000 (97%)]\t1.520114\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy 58486/60000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\t1.513855\n",
            "Train Epoch: 6 [2000/60000 (3%)]\t1.555094\n",
            "Train Epoch: 6 [4000/60000 (7%)]\t1.551429\n",
            "Train Epoch: 6 [6000/60000 (10%)]\t1.546002\n",
            "Train Epoch: 6 [8000/60000 (13%)]\t1.536186\n",
            "Train Epoch: 6 [10000/60000 (17%)]\t1.492179\n",
            "Train Epoch: 6 [12000/60000 (20%)]\t1.487460\n",
            "Train Epoch: 6 [14000/60000 (23%)]\t1.503443\n",
            "Train Epoch: 6 [16000/60000 (27%)]\t1.543698\n",
            "Train Epoch: 6 [18000/60000 (30%)]\t1.542060\n",
            "Train Epoch: 6 [20000/60000 (33%)]\t1.560767\n",
            "Train Epoch: 6 [22000/60000 (37%)]\t1.522795\n",
            "Train Epoch: 6 [24000/60000 (40%)]\t1.519881\n",
            "Train Epoch: 6 [26000/60000 (43%)]\t1.541399\n",
            "Train Epoch: 6 [28000/60000 (47%)]\t1.502237\n",
            "Train Epoch: 6 [30000/60000 (50%)]\t1.532081\n",
            "Train Epoch: 6 [32000/60000 (53%)]\t1.547519\n",
            "Train Epoch: 6 [34000/60000 (57%)]\t1.511145\n",
            "Train Epoch: 6 [36000/60000 (60%)]\t1.490187\n",
            "Train Epoch: 6 [38000/60000 (63%)]\t1.489406\n",
            "Train Epoch: 6 [40000/60000 (67%)]\t1.504933\n",
            "Train Epoch: 6 [42000/60000 (70%)]\t1.521213\n",
            "Train Epoch: 6 [44000/60000 (73%)]\t1.527713\n",
            "Train Epoch: 6 [46000/60000 (77%)]\t1.498251\n",
            "Train Epoch: 6 [48000/60000 (80%)]\t1.495712\n",
            "Train Epoch: 6 [50000/60000 (83%)]\t1.518499\n",
            "Train Epoch: 6 [52000/60000 (87%)]\t1.505684\n",
            "Train Epoch: 6 [54000/60000 (90%)]\t1.479978\n",
            "Train Epoch: 6 [56000/60000 (93%)]\t1.517931\n",
            "Train Epoch: 6 [58000/60000 (97%)]\t1.514790\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy 58611/60000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\t1.560266\n",
            "Train Epoch: 7 [2000/60000 (3%)]\t1.521286\n",
            "Train Epoch: 7 [4000/60000 (7%)]\t1.516618\n",
            "Train Epoch: 7 [6000/60000 (10%)]\t1.512350\n",
            "Train Epoch: 7 [8000/60000 (13%)]\t1.529974\n",
            "Train Epoch: 7 [10000/60000 (17%)]\t1.500215\n",
            "Train Epoch: 7 [12000/60000 (20%)]\t1.518800\n",
            "Train Epoch: 7 [14000/60000 (23%)]\t1.486974\n",
            "Train Epoch: 7 [16000/60000 (27%)]\t1.500871\n",
            "Train Epoch: 7 [18000/60000 (30%)]\t1.464350\n",
            "Train Epoch: 7 [20000/60000 (33%)]\t1.529263\n",
            "Train Epoch: 7 [22000/60000 (37%)]\t1.510896\n",
            "Train Epoch: 7 [24000/60000 (40%)]\t1.530885\n",
            "Train Epoch: 7 [26000/60000 (43%)]\t1.541253\n",
            "Train Epoch: 7 [28000/60000 (47%)]\t1.507408\n",
            "Train Epoch: 7 [30000/60000 (50%)]\t1.490644\n",
            "Train Epoch: 7 [32000/60000 (53%)]\t1.515669\n",
            "Train Epoch: 7 [34000/60000 (57%)]\t1.505651\n",
            "Train Epoch: 7 [36000/60000 (60%)]\t1.545822\n",
            "Train Epoch: 7 [38000/60000 (63%)]\t1.505205\n",
            "Train Epoch: 7 [40000/60000 (67%)]\t1.515058\n",
            "Train Epoch: 7 [42000/60000 (70%)]\t1.527009\n",
            "Train Epoch: 7 [44000/60000 (73%)]\t1.503576\n",
            "Train Epoch: 7 [46000/60000 (77%)]\t1.515223\n",
            "Train Epoch: 7 [48000/60000 (80%)]\t1.513878\n",
            "Train Epoch: 7 [50000/60000 (83%)]\t1.504903\n",
            "Train Epoch: 7 [52000/60000 (87%)]\t1.493872\n",
            "Train Epoch: 7 [54000/60000 (90%)]\t1.486320\n",
            "Train Epoch: 7 [56000/60000 (93%)]\t1.510700\n",
            "Train Epoch: 7 [58000/60000 (97%)]\t1.488047\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy 58803/60000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\t1.520610\n",
            "Train Epoch: 8 [2000/60000 (3%)]\t1.512857\n",
            "Train Epoch: 8 [4000/60000 (7%)]\t1.490056\n",
            "Train Epoch: 8 [6000/60000 (10%)]\t1.504026\n",
            "Train Epoch: 8 [8000/60000 (13%)]\t1.513345\n",
            "Train Epoch: 8 [10000/60000 (17%)]\t1.496970\n",
            "Train Epoch: 8 [12000/60000 (20%)]\t1.515413\n",
            "Train Epoch: 8 [14000/60000 (23%)]\t1.502995\n",
            "Train Epoch: 8 [16000/60000 (27%)]\t1.486739\n",
            "Train Epoch: 8 [18000/60000 (30%)]\t1.477082\n",
            "Train Epoch: 8 [20000/60000 (33%)]\t1.549602\n",
            "Train Epoch: 8 [22000/60000 (37%)]\t1.502012\n",
            "Train Epoch: 8 [24000/60000 (40%)]\t1.523756\n",
            "Train Epoch: 8 [26000/60000 (43%)]\t1.494092\n",
            "Train Epoch: 8 [28000/60000 (47%)]\t1.484376\n",
            "Train Epoch: 8 [30000/60000 (50%)]\t1.518093\n",
            "Train Epoch: 8 [32000/60000 (53%)]\t1.493479\n",
            "Train Epoch: 8 [34000/60000 (57%)]\t1.512203\n",
            "Train Epoch: 8 [36000/60000 (60%)]\t1.474223\n",
            "Train Epoch: 8 [38000/60000 (63%)]\t1.530961\n",
            "Train Epoch: 8 [40000/60000 (67%)]\t1.505626\n",
            "Train Epoch: 8 [42000/60000 (70%)]\t1.509952\n",
            "Train Epoch: 8 [44000/60000 (73%)]\t1.522983\n",
            "Train Epoch: 8 [46000/60000 (77%)]\t1.509503\n",
            "Train Epoch: 8 [48000/60000 (80%)]\t1.501785\n",
            "Train Epoch: 8 [50000/60000 (83%)]\t1.485493\n",
            "Train Epoch: 8 [52000/60000 (87%)]\t1.517629\n",
            "Train Epoch: 8 [54000/60000 (90%)]\t1.507428\n",
            "Train Epoch: 8 [56000/60000 (93%)]\t1.498262\n",
            "Train Epoch: 8 [58000/60000 (97%)]\t1.479077\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy 58865/60000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\t1.493305\n",
            "Train Epoch: 9 [2000/60000 (3%)]\t1.492784\n",
            "Train Epoch: 9 [4000/60000 (7%)]\t1.502146\n",
            "Train Epoch: 9 [6000/60000 (10%)]\t1.481010\n",
            "Train Epoch: 9 [8000/60000 (13%)]\t1.519511\n",
            "Train Epoch: 9 [10000/60000 (17%)]\t1.530406\n",
            "Train Epoch: 9 [12000/60000 (20%)]\t1.521208\n",
            "Train Epoch: 9 [14000/60000 (23%)]\t1.510179\n",
            "Train Epoch: 9 [16000/60000 (27%)]\t1.491131\n",
            "Train Epoch: 9 [18000/60000 (30%)]\t1.528684\n",
            "Train Epoch: 9 [20000/60000 (33%)]\t1.495519\n",
            "Train Epoch: 9 [22000/60000 (37%)]\t1.499259\n",
            "Train Epoch: 9 [24000/60000 (40%)]\t1.466359\n",
            "Train Epoch: 9 [26000/60000 (43%)]\t1.511544\n",
            "Train Epoch: 9 [28000/60000 (47%)]\t1.529481\n",
            "Train Epoch: 9 [30000/60000 (50%)]\t1.485200\n",
            "Train Epoch: 9 [32000/60000 (53%)]\t1.515054\n",
            "Train Epoch: 9 [34000/60000 (57%)]\t1.522005\n",
            "Train Epoch: 9 [36000/60000 (60%)]\t1.504537\n",
            "Train Epoch: 9 [38000/60000 (63%)]\t1.526515\n",
            "Train Epoch: 9 [40000/60000 (67%)]\t1.477247\n",
            "Train Epoch: 9 [42000/60000 (70%)]\t1.516303\n",
            "Train Epoch: 9 [44000/60000 (73%)]\t1.488982\n",
            "Train Epoch: 9 [46000/60000 (77%)]\t1.501240\n",
            "Train Epoch: 9 [48000/60000 (80%)]\t1.517056\n",
            "Train Epoch: 9 [50000/60000 (83%)]\t1.494786\n",
            "Train Epoch: 9 [52000/60000 (87%)]\t1.524590\n",
            "Train Epoch: 9 [54000/60000 (90%)]\t1.536927\n",
            "Train Epoch: 9 [56000/60000 (93%)]\t1.516630\n",
            "Train Epoch: 9 [58000/60000 (97%)]\t1.491411\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy 58746/60000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\t1.477098\n",
            "Train Epoch: 10 [2000/60000 (3%)]\t1.519722\n",
            "Train Epoch: 10 [4000/60000 (7%)]\t1.471427\n",
            "Train Epoch: 10 [6000/60000 (10%)]\t1.501971\n",
            "Train Epoch: 10 [8000/60000 (13%)]\t1.517760\n",
            "Train Epoch: 10 [10000/60000 (17%)]\t1.536085\n",
            "Train Epoch: 10 [12000/60000 (20%)]\t1.489302\n",
            "Train Epoch: 10 [14000/60000 (23%)]\t1.499442\n",
            "Train Epoch: 10 [16000/60000 (27%)]\t1.500994\n",
            "Train Epoch: 10 [18000/60000 (30%)]\t1.475075\n",
            "Train Epoch: 10 [20000/60000 (33%)]\t1.516060\n",
            "Train Epoch: 10 [22000/60000 (37%)]\t1.497061\n",
            "Train Epoch: 10 [24000/60000 (40%)]\t1.497817\n",
            "Train Epoch: 10 [26000/60000 (43%)]\t1.511333\n",
            "Train Epoch: 10 [28000/60000 (47%)]\t1.472179\n",
            "Train Epoch: 10 [30000/60000 (50%)]\t1.494656\n",
            "Train Epoch: 10 [32000/60000 (53%)]\t1.498205\n",
            "Train Epoch: 10 [34000/60000 (57%)]\t1.502789\n",
            "Train Epoch: 10 [36000/60000 (60%)]\t1.481539\n",
            "Train Epoch: 10 [38000/60000 (63%)]\t1.514158\n",
            "Train Epoch: 10 [40000/60000 (67%)]\t1.495714\n",
            "Train Epoch: 10 [42000/60000 (70%)]\t1.499344\n",
            "Train Epoch: 10 [44000/60000 (73%)]\t1.504168\n",
            "Train Epoch: 10 [46000/60000 (77%)]\t1.474177\n",
            "Train Epoch: 10 [48000/60000 (80%)]\t1.500119\n",
            "Train Epoch: 10 [50000/60000 (83%)]\t1.495294\n",
            "Train Epoch: 10 [52000/60000 (87%)]\t1.515962\n",
            "Train Epoch: 10 [54000/60000 (90%)]\t1.497165\n",
            "Train Epoch: 10 [56000/60000 (93%)]\t1.489447\n",
            "Train Epoch: 10 [58000/60000 (97%)]\t1.506095\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy 58866/60000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary library for plotting images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the model to evaluation mode (important for layers like dropout/batchnorm to behave differently during testing)\n",
        "model.eval()\n",
        "\n",
        "# Get a sample image and its target label (true class) from the test data\n",
        "data, target = test_data[0]  # Here, `test_data[0]` fetches the 1st sample (index starts at 0) Make sure to experiment and try different samples: like test_data[5] etc\n",
        "\n",
        "# Add an extra dimension to the image tensor so that it's in the shape (1, 1, 28, 28),\n",
        "# where 1 is for batch size and the second 1 is for the channel (grayscale image).\n",
        "# Then, move the image to the correct device (GPU or CPU) for prediction.\n",
        "data = data.unsqueeze(0).to(device)\n",
        "\n",
        "# Pass the image through the model to get the predicted output\n",
        "output = model(data)\n",
        "\n",
        "# Get the predicted class by finding the index of the highest predicted value (this represents the class with the highest score)\n",
        "# `argmax` finds the index of the maximum value in the output tensor (predicted class).\n",
        "prediction = output.argmax(dim=1, keepdim=True).item()\n",
        "\n",
        "# Print the predicted class\n",
        "print(f\"Prediction: {prediction}\")\n",
        "\n",
        "# Remove the extra dimensions we added earlier (to match the shape of the original image),\n",
        "# and move the data back to the CPU for plotting.\n",
        "# `.squeeze(0)` removes the first dimension (batch size), `.squeeze(0)` removes the second dimension (channel).\n",
        "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Use matplotlib to display the image, with the color map set to 'gray' for grayscale images\n",
        "plt.imshow(image, cmap='gray')\n",
        "\n",
        "# Show the image in a window\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "8xxZ_tQ6HuW2",
        "outputId": "4f4c341a-a26c-4ba7-8960-0624ff5176b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}